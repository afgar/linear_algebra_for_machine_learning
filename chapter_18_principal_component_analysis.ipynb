{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 18. Análisis de Componentes Principales\n",
    "\n",
    "En el aprendizaje automático es un método que se utiliza para la reducción de la dimensionalidad. Utiliza operaciones simples de matriz de álgebra lineal y estadísticas para calcular una proyección de los datos originales en el mismo número o menos dimensiones.\n",
    "\n",
    "## ¿Qué es?\n",
    "\n",
    "Principal Component Analysis, o PCA para abreviar, es un método para reducir la dimensionalidad de los datos. Se puede considerar como un método de proyección donde los datos con m-columns (características) se proyectan en un subespacio con m o menos columnas, mientras se conserva la esencia de los datos originales. El método PCA puede describirse e implementarse utilizando las herramientas del álgebra lineal.\n",
    "\n",
    "PCA es una operación aplicada a un conjunto de datos, representada por una matriz n × m A que da como resultado una proyección de A que llamaremos B. Vamos a seguir los pasos de esta operación.\n",
    "\n",
    "`B = PCA(A)`\n",
    "\n",
    "El primer paso es calcular los valores medios de cada columna\n",
    "\n",
    "`M = mean(A)`\n",
    "\n",
    "A continuación, necesitamos centrar los valores de cada columna sustrayendo la media de cada valor de la columna.\n",
    "\n",
    "`C = A − M`\n",
    "\n",
    "El siguiente paso es calcular la matriz de covarianza de la matriz centrada C. La correlación es una medida normalizada de la cantidad y dirección (positiva o negativa) en que dos columnas cambian juntas. La covarianza es una versión generalizada y no normalizada de la correlación en varias columnas. Una matriz de covarianza es un cálculo de covarianza de una matriz dada con puntajes de covarianza para cada columna con cada otra columna, incluido ella misma.\n",
    "\n",
    "`V = cov(C)`\n",
    "\n",
    "Finalmente, calculamos la descomposición propia (eigendecomposition) de la matriz de covarianza V. Esto da como resultado una lista de valores propios (eigenvalues) y una lista de vectores propios (eigenvectors).\n",
    "\n",
    "`values, vectors = eig(V)`\n",
    "\n",
    "Los vectores propios representan las direcciones o componentes para el subespacio reducido de B, mientras que los valores propios representan las magnitudes de las direcciones. Los vectores propios se pueden ordenar por valores propios en orden descendente para proporcionar una clasificación de los componentes o ejes del nuevo subespacio para A. Si todos los valores propios tienen un valor similar, entonces sabemos que la representación existente puede estar razonablemente comprimida o es densa y que la proyección puede ofrecer poco. Si hay valores propios cercanos a cero, representan componentes o ejes de B que pueden descartarse. Se debe seleccionar un total de m o menos componentes para que comprendan el subespacio elegido. Idealmente, seleccionaríamos k vectores propios, llamados componentes principales, que tienen los k valores propios mas grandes.\n",
    "\n",
    "`B = select(values, vectors)`\n",
    "\n",
    "Se pueden usar otros métodos de descomposición de la matriz, como Descomposición de valores singulares o SVD. Como tal, generalmente los valores se denominan valores singulares y los vectores del subespacio se denominan componentes principales. Una vez elegidos, los datos pueden proyectarse en el subespacio a través de la multiplicación de la matriz.\n",
    "\n",
    "P = B<sup>T</sup> · A\n",
    "\n",
    "Donde A es los datos originales que deseamos proyectar, B es la transposición de los componentes principales elegidos y P es la proyección de A. Esto se conoce como la el método de covarianza para calcular el PCA, aunque existen formas alternativas de calcularlo.\n",
    "\n",
    "## Cálculo del Análisis de Componentes Principales\n",
    "\n",
    "No hay función pca() en NumPy, pero podemos calcular fácilmente el Análisis de Componentes Principales paso a paso usando las funciones de NumPy. El siguiente ejemplo define una pequeña matriz de 3 × 2, centra los datos en la matriz, calcula la matriz de covarianza de los datos centrados y luego la descomposición propia de la matriz de covarianza. Los vectores propios y valores propios se toman como componentes principales y valores singulares y se utilizan para proyectar los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 4.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# principal component analysis\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "\n",
    "# column means\n",
    "M = mean(A.T, axis=1)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -2.],\n",
       "       [ 0.,  0.],\n",
       "       [ 2.,  2.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4.],\n",
       "       [4., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factorize covariance matrix\n",
    "values, vectors = eig(V)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 2.82842712,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "P.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiosamente, podemos ver que solo se necesita el primer vector propio, lo que sugiere que podríamos proyectar nuestra matriz de 3 × 2 en una matriz de 3 × 1 con poca pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Componentes Principales en scikit-learn\n",
    "\n",
    "Podemos calcular el Análisis de Componentes Principales en un dataset utilizando la clase PCA() en la biblioteca scikit-learn. El beneficio de este enfoque es que una vez que se calcula la proyección, se puede aplicar a los datos nuevos una y otra vez con bastante facilidad. Al crear la clase, la cantidad de componentes se puede especificar como parámetro. La clase se ajusta por primera vez al dataset llamando a la función fit(), y luego el dataset original u otros datos pueden proyectarse en un subespacio con el número elegido de dimensiones llamando a la función transform(). Una vez que estén ajustados, se puede acceder a los valores propios y componentes principales en la clase PCA a través de los atributos explained_variance_ y components_. El siguiente ejemplo demuestra el uso de esta clase creando primero una instancia, ajustándola en una matriz 3 × 2, accediendo a los valores y vectores de la proyección, y transformando los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# principal component analysis with scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# create the transform\n",
    "pca = PCA(2)\n",
    "\n",
    "# fit transform\n",
    "pca.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.70710678],\n",
       "       [ 0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access values and vectors\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.00000000e+00, 2.25080839e-33])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712e+00,  2.22044605e-16],\n",
       "       [ 0.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.82842712e+00, -2.22044605e-16]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data\n",
    "B = pca.transform(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
